{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Leave-One-Out Cross Validation\n",
    "\n",
    "We'll perform LOOCV using our modified LINCS Random Forest implementation and test the model's accuracy on each compounds after training on the remaining compounds. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from support_functions import log_progress\n",
    "import scipy\n",
    "import time\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_batches = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cddb8ac854b4f82aefd8368e65a8039"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Construct metadata table\n",
    "cpd_kd_pair_df = pd.DataFrame(columns=['cpd', 'kd', 'label'])\n",
    "\n",
    "for b in log_progress(range(num_batches)):\n",
    "    df_filename = 'checkpoint_files/pair_set_4_subdf_{}.csv'.format(b)\n",
    "    cpd_kd_pair_df = cpd_kd_pair_df.append(pd.DataFrame.from_csv(df_filename))\n",
    "    \n",
    "cpd_kd_pair_df = cpd_kd_pair_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile of training data: \n",
      "\n",
      "No. True interactions:\t 592\n",
      "No. True cpds:\t\t 182\n",
      "No. True genes:\t\t 250\n",
      "\n",
      "No. False interactions:\t 2688751\n",
      "No. False cpds:\t\t 933\n",
      "No. False genes:\t 3233\n"
     ]
    }
   ],
   "source": [
    "print('Profile of training data: \\n')\n",
    "print('No. True interactions:\\t', cpd_kd_pair_df.label.value_counts()[1])\n",
    "print('No. True cpds:\\t\\t', len(cpd_kd_pair_df[cpd_kd_pair_df.label == 1].cpd.unique()))\n",
    "print('No. True genes:\\t\\t', len(cpd_kd_pair_df[cpd_kd_pair_df.label == 1].kd.unique()))\n",
    "print('\\nNo. False interactions:\\t', cpd_kd_pair_df.label.value_counts()[0])\n",
    "print('No. False cpds:\\t\\t', len(cpd_kd_pair_df[cpd_kd_pair_df.label == 0].cpd.unique()))\n",
    "print('No. False genes:\\t', len(cpd_kd_pair_df[cpd_kd_pair_df.label == 0].kd.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c7ea368e204401bd2aed47781e9dd9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Direct Correlation Data\n",
    "dir_corr_df_0 = pd.DataFrame.from_csv('features/top_7_dir_corr_sub_df_0.csv')\n",
    "dir_corr_df = pd.DataFrame(columns=dir_corr_df_0.columns)\n",
    "dir_corr_df = dir_corr_df.append(dir_corr_df_0)\n",
    "\n",
    "for b in log_progress(range(1,num_batches)):\n",
    "    df_filename = 'features/top_7_dir_corr_sub_df_{}.csv'.format(b)\n",
    "    dir_corr_df = dir_corr_df.append(pd.DataFrame.from_csv(df_filename))\n",
    "    \n",
    "dir_corr_df = dir_corr_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdf28ca1a194188afcd178535cdced9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Indirect Correlation Data\n",
    "indir_max_corr_df_0 = pd.DataFrame.from_csv('features/top_7_indir_max_corr_sub_df_0.csv')\n",
    "indir_max_corr_df = pd.DataFrame(columns=indir_max_corr_df_0.columns)\n",
    "indir_max_corr_df = indir_max_corr_df.append(indir_max_corr_df_0)\n",
    "\n",
    "indir_min_corr_df_0 = pd.DataFrame.from_csv('features/top_7_indir_min_corr_sub_df_0.csv')\n",
    "indir_min_corr_df = pd.DataFrame(columns=indir_min_corr_df_0.columns)\n",
    "indir_min_corr_df = indir_min_corr_df.append(indir_min_corr_df_0)\n",
    "\n",
    "indir_avg_corr_df_0 = pd.DataFrame.from_csv('features/top_7_indir_avg_corr_sub_df_0.csv')\n",
    "indir_avg_corr_df = pd.DataFrame(columns=indir_avg_corr_df_0.columns)\n",
    "indir_avg_corr_df = indir_avg_corr_df.append(indir_avg_corr_df_0)\n",
    "\n",
    "\n",
    "for b in log_progress(range(1,num_batches)):\n",
    "    max_df_filename = 'features/top_7_indir_max_corr_sub_df_{}.csv'.format(b)\n",
    "    min_df_filename = 'features/top_7_indir_min_corr_sub_df_{}.csv'.format(b)\n",
    "    avg_df_filename = 'features/top_7_indir_avg_corr_sub_df_{}.csv'.format(b)\n",
    "    \n",
    "    indir_max_corr_df = indir_max_corr_df.append(pd.DataFrame.from_csv(max_df_filename))\n",
    "    indir_min_corr_df = indir_min_corr_df.append(pd.DataFrame.from_csv(min_df_filename))\n",
    "    indir_avg_corr_df = indir_avg_corr_df.append(pd.DataFrame.from_csv(avg_df_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea576f38532c4952af270248fe7106a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile features into dataframe\n",
    "cell_lines = dir_corr_df.columns\n",
    "X_df = pd.DataFrame()\n",
    "\n",
    "for cell_line in log_progress(cell_lines):\n",
    "    dir_corr = dir_corr_df[cell_line].values\n",
    "    indir_max_corr = indir_max_corr_df[cell_line].values\n",
    "    indir_min_corr = indir_min_corr_df[cell_line].values\n",
    "    indir_avg_corr = indir_avg_corr_df[cell_line].values\n",
    "  \n",
    "    tmp_df = pd.DataFrame({'{}_dir'.format(cell_line): dir_corr,\n",
    "                           '{}_max'.format(cell_line): indir_max_corr,\n",
    "                           '{}_min'.format(cell_line): indir_min_corr,\n",
    "                           '{}_avg'.format(cell_line): indir_avg_corr,\n",
    "                          })\n",
    "    \n",
    "    X_df = pd.concat([X_df, tmp_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what is the minimum number of cells we will use for training/testing\n",
    "min_num_cells = 4\n",
    "\n",
    "# filter the data to select samples with sufficient data\n",
    "n_cells = 7 - np.count_nonzero(dir_corr_df.isnull().astype(int), axis=1)\n",
    "cpd_kd_pair_df['n_cells'] = n_cells\n",
    "suff_data_idx = cpd_kd_pair_df.n_cells >= min_num_cells\n",
    "\n",
    "filtered_cpd_kd_pair_df = cpd_kd_pair_df[suff_data_idx].copy().reset_index(drop=True)\n",
    "filtered_X_df = X_df[suff_data_idx].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# which compounds have sufficient data for known targets\n",
    "pos_cpds = filtered_cpd_kd_pair_df[filtered_cpd_kd_pair_df.label == 1].cpd.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cda9a8b1f5436b90610897a31f08bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6c3ec7ac6c4060ab70834419d426e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164d7483bfeb4b73a1987db383baf6f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split up data into training and validation\n",
    "loocv_result_df = pd.DataFrame()\n",
    "\n",
    "for cpd in log_progress(pos_cpds):\n",
    "    \n",
    "    # split train/test, testing 1 cpd at a time\n",
    "    test_idx = filtered_cpd_kd_pair_df[filtered_cpd_kd_pair_df.cpd == cpd].index\n",
    "    train_idx = filtered_cpd_kd_pair_df[filtered_cpd_kd_pair_df.cpd != cpd].index\n",
    "    X_test = filtered_X_df.loc[test_idx].values\n",
    "    X_train = filtered_X_df.loc[train_idx].values\n",
    "    y_train = filtered_cpd_kd_pair_df.loc[train_idx].label.values.astype(int)\n",
    "    \n",
    "    # train the model\n",
    "    LRF = LincsRandomForestClassifier(n_cells_per_forest = min_num_cells,\n",
    "                                      n_estimators_per_forest=100, \n",
    "                                      max_depth=12, \n",
    "                                      max_features=\"auto\",\n",
    "                                      class_weight=\"balanced_subsample\",\n",
    "                                      random_state=1)\n",
    "    LRF.fit(X_train, y_train)\n",
    "    \n",
    "    # predict probabilities for test cpd's potential targets\n",
    "    test_proba_ = LRF.predict_proba_(X_test)\n",
    "    \n",
    "    # rank potential targets predicted probability\n",
    "    test_result_df = cpd_kd_pair_df.loc[test_idx].copy()\n",
    "    test_result_df['proba'] = test_proba_[:,1]\n",
    "    test_result_df['rank'] = test_result_df.proba.rank(ascending=False)\n",
    "    test_result_df['n_potential_targets'] = len(test_result_df)\n",
    "\n",
    "    # record rank of true target(s) and which cell lines had data\n",
    "    true_target_result = test_result_df[test_result_df.label == 1]\n",
    "    true_target_cell_lines = ~dir_corr_df.loc[true_target_result.index].isnull()\n",
    "    loo_result = pd.concat([true_target_result, true_target_cell_lines], axis=1)\n",
    "\n",
    "    # record results\n",
    "    loocv_result_df = loocv_result_df.append(loo_result)\n",
    "    loocv_result_df.to_csv('results/loocv_results_min4_tree100_depth12_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LincsRandomForestClassifier(object):\n",
    "    \n",
    "    \"\"\"WE ASSUME THE DATA IS GROUPED BY CELL LINE AND HAS 4 FEATURES PER CELL LINE\"\"\"\n",
    "   \n",
    "    def __init__(self, n_cells_per_forest, \n",
    "                 n_estimators_per_forest=10, \n",
    "                 max_depth=None, \n",
    "                 max_features=\"auto\",\n",
    "                 class_weight=\"balanced_subsample\",\n",
    "                 random_state=1,):\n",
    "        self.n_cells_per_forest = n_cells_per_forest\n",
    "        self.n_estimators_per_forest = n_estimators_per_forest\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.class_weight = class_weight\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Train several random forests, each one on a different\n",
    "        subset of cells. Store forests in a dictionary called\n",
    "        self.forests.\n",
    "        '''\n",
    "        # make sure we have enough data to work with\n",
    "        min_num_cells = self.get_min_num_cells(X)\n",
    "        assert min_num_cells >= self.n_cells_per_forest, \"Too much missing data for n_cells_per_forest = %s. (Some samples only tested in %d cells)\" % \\\n",
    "                                                         (self.n_cells_per_forest, min_num_cells)\n",
    "        \n",
    "        # generate cell subsets for training\n",
    "        # ASSUMES 4 FEATURES PER CELL\n",
    "        total_num_cells = int(X.shape[1] / 4) # THIS IS HARDCODED IN\n",
    "        cell_subsets = itertools.combinations(np.arange(total_num_cells), self.n_cells_per_forest)\n",
    "        \n",
    "        # initialize dictionary to hold the forests\n",
    "        self.forests = {}\n",
    "        \n",
    "        # train forest on each subset\n",
    "        for cell_subset in log_progress(cell_subsets, every=1):\n",
    "            #print('Growing forest for cell lines: ', cell_subset, end=\"\\t\")\n",
    "            \n",
    "            # find samples that have complete data from the cell subset\n",
    "            cell_subset_idx = np.array([ 4*i + np.array([0, 1, 2, 3])for i in cell_subset ]).reshape(1,-1)[0].astype(int)\n",
    "            cell_subset_data = X[:,cell_subset_idx]\n",
    "            bad_sample_idx = np.isnan(cell_subset_data).any(axis=1)\n",
    "            good_samples = cell_subset_data[~bad_sample_idx]\n",
    "            good_labels = y[~bad_sample_idx]\n",
    "            #print('Sample class distribution: ', np.bincount(good_labels.astype(int)))\n",
    "            \n",
    "            # train and store a RF classifier on this training subset\n",
    "            # print('Growing forest for cell subset: %s' % str(cell_subset))\n",
    "            forest = RandomForestClassifier(criterion='gini',\n",
    "                                            n_estimators=self.n_estimators_per_forest,\n",
    "                                            max_depth=self.max_depth,\n",
    "                                            max_features=self.max_features,\n",
    "                                            class_weight=self.class_weight,\n",
    "                                            random_state=self.random_state,\n",
    "                                            n_jobs=-1)\n",
    "            forest.fit(good_samples, good_labels)\n",
    "            self.forests[cell_subset] = forest            \n",
    "\n",
    "        \n",
    "    def get_min_num_cells(self, X):\n",
    "        '''\n",
    "        Calculate the minimum number of cells any sample has data for\n",
    "        ASSUMES 4 FEATURES PER CELL LINE\n",
    "        '''\n",
    "        X_not_missing = ~np.isnan(X)\n",
    "        num_cells_not_missing = np.count_nonzero(X_not_missing, axis=1) / 4\n",
    "        min_num_cells = np.min(num_cells_not_missing)\n",
    "        return min_num_cells\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Return the class probabilities label OF ONE SINGLE SAMPLE \n",
    "        '''\n",
    "        # figure out which cell lines we have data for\n",
    "        non_nan_idx = np.where(np.isnan(X) == False)[0]\n",
    "        good_cells = (non_nan_idx[np.where(non_nan_idx/4%1 == 0)[0]] / 4).astype(int)\n",
    "        # select appropriate forests and predict\n",
    "        cell_subsets = itertools.combinations(good_cells, self.n_cells_per_forest)\n",
    "        tree_predictions_ = []\n",
    "        for cell_subset in cell_subsets:\n",
    "            # extract appropriate data\n",
    "            cell_subset_idx = np.array([ 4*i + np.array([0, 1, 2, 3])for i in cell_subset ]).reshape(1,-1)[0].astype(int)\n",
    "            cell_subset_data = X[cell_subset_idx].reshape(1,-1) \n",
    "            # extract appropriate forest and make prediction\n",
    "            forest = self.forests[cell_subset]\n",
    "            tree_predictions = [ tree.predict(cell_subset_data) for tree in forest.estimators_ ]\n",
    "            tree_predictions_.append(tree_predictions)\n",
    "        \n",
    "        # majority vote of all the trees in all the forests\n",
    "        results = np.array(tree_predictions_).flatten()\n",
    "        proba = results.sum() / len(results)\n",
    "        return np.array([1.-proba, proba])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Return the predicted class label OF ONE SINGLE SAMPLE\n",
    "        '''\n",
    "        class_probabilities = self.predict_proba(X)\n",
    "        return np.argmax(class_probabilities)\n",
    "    \n",
    "    \n",
    "    def predict_proba_(self, X):\n",
    "        proba_ = []\n",
    "        for i in range(len(X)):\n",
    "            proba_.append(self.predict_proba(X[i]))\n",
    "        return np.array(proba_)\n",
    "    \n",
    "    def predict_(self, X):\n",
    "        '''\n",
    "        for a multidimentional X\n",
    "        '''\n",
    "        predicted_classes = np.array([ self.predict(x) for x in X ])\n",
    "        return predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
